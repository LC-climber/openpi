<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pi0.6 与 RECAP 算法原理解析</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/plugin/math/math.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reset.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/theme/black.min.css">
    
    <!-- Custom Styles -->
    <style>
        .reveal h1, .reveal h2, .reveal h3 {
            text-transform: none;
            color: #61dafb;
        }
        .reveal p, .reveal li {
            font-size: 0.85em;
            line-height: 1.5;
            color: #e0e0e0;
        }
        .highlight {
            color: #ffcc00;
            font-weight: bold;
        }
        .math-box {
            background: rgba(255, 255, 255, 0.05);
            padding: 20px;
            border-radius: 10px;
            border-left: 5px solid #61dafb;
            margin: 20px 0;
            font-size: 0.8em;
            text-align: left;
        }
        .logic-step {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-top: 20px;
        }
        .logic-box {
            background: #2b2b2b;
            padding: 15px;
            border-radius: 8px;
            width: 30%;
            font-size: 0.7em;
            box-shadow: 0 4px 6px rgba(0,0,0,0.3);
        }
        .arrow {
            font-size: 2em;
            color: #61dafb;
        }
        .two-col {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
        }
        .small-text {
            font-size: 0.6em !important;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">

            <!-- Title Slide -->
            <section>
                <h2>从 $\pi$ 到 $\pi_{0.6}$</h2>
                <h3>基于 RECAP 的 VLA 模型进化之路</h3>
                <p style="margin-top: 50px; color: #888;">基于 Physical Intelligence 论文: "$\pi_{0.6}$: a VLA That Learns From Experience"</p>
            </section>

            <!-- Q1: Pi Series Origin -->
            <section>
                <h3>1. 为什么会出现 $\pi$ 系列?</h3>
                <div class="two-col">
                    <div>
                        <ul>
                            <li><strong>通用机器人基础模型 (VLA):</strong> 目标是创建一个像 ChatGPT 一样的机器人大脑，能通过<strong>视觉-语言-动作</strong>理解并执行任务。</li>
                            <li><strong>零样本/少样本能力:</strong> 通过大规模预训练，只需简单的提示（Prompt）即可让机器人执行未见过的任务。</li>
                        </ul>
                    </div>
                    <div style="display:flex; align-items:center; justify-content:center;">
                        <div style="background:#333; padding:20px; border-radius:10px;">
                            <p>📷 视觉输入 + <br>💬 "把咖啡倒进杯子" <br>⬇️<br> 🤖 机械臂轨迹</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Q2: Why Pi0.6 -->
            <section>
                <h3>2. $\pi_{0.6}$ 基于什么背景提出?</h3>
                <p class="highlight">"熟能生巧 (Practice makes perfect)"</p>
                <ul>
                    <li><strong>模仿学习 (Imitation Learning) 的局限:</strong> 之前的 $\pi_0$ 主要是模仿人类示教数据。</li>
                    <li><strong>瓶颈:</strong> 
                        <ul>
                            <li>模型表现永远无法超过示教者。</li>
                            <li>累积误差 (Compounding Errors): 一步错，步步错。</li>
                        </ul>
                    </li>
                    <li><strong>需求:</strong> 机器人需要像人类一样，通过<strong>不断的练习和失败</strong>（强化学习 RL）来精进技能，而不仅仅是模仿。</li>
                </ul>
            </section>

            <!-- Q3: Existing Solutions & Defects -->
            <section>
                <h3>3. 现有解决办法与缺陷</h3>
                <table style="font-size: 0.6em; border-collapse: collapse; width: 100%;">
                    <thead>
                        <tr style="border-bottom: 2px solid #61dafb;">
                            <th>方法</th>
                            <th>原理</th>
                            <th>缺陷</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>PPO (近端策略优化)</strong></td>
                            <td>经典的在线 RL 算法</td>
                            <td>对于巨型 VLA 模型来说<strong>太不稳定，难以扩展</strong>，且训练计算成本极高。</td>
                        </tr>
                        <tr>
                            <td><strong>AWR (优势加权回归)</strong></td>
                            <td>给好数据高权重，坏数据低权重</td>
                            <td>本质是<strong>"过滤后的模仿"</strong>，会丢弃大量"次优"但有用的数据，数据利用率低。</td>
                        </tr>
                        <tr>
                            <td><strong>Residual RL</strong></td>
                            <td>训练一个小模型修补大模型</td>
                            <td>无法端到端提升 VLA 本体的能力，治标不治本。</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <!-- Q4: Why Pi0.6 is Good -->
            <section>
                <h3>4. $\pi_{0.6}$ 好在哪里?</h3>
                <ul>
                    <li><strong>全能与专精结合:</strong> 既保持了通用 VLA 的能力，又能在特定任务（如做咖啡、叠衣服）上通过练习达到<strong>超越人类遥操作</strong>的速度和稳定性。</li>
                    <li><strong>RECAP 方法:</strong> 一种通用的、可扩展的 RL 微调配方。</li>
                    <li><strong>数据利用率高:</strong> 能利用所有数据——成功的、失败的、人类修正的（Interventions）。</li>
                    <li><strong>抗干扰强:</strong> 在 13 小时不间断做咖啡测试中表现极佳。</li>
                </ul>
            </section>

            <!-- Q5: Technical Deep Dive - Overview -->
            <section>
                <h3>5. $\pi_{0.6}$ 如何变强? (核心原理)</h3>
                <p class="highlight">RECAP: RL with Experience and Corrections via Advantage-conditioned Policies</p>
                <div class="logic-step">
                    <div class="logic-box">
                        <h4>1. 收集数据</h4>
                        <p>自主尝试 + 专家接管修正 (Interventions)</p>
                    </div>
                    <div class="arrow">➡</div>
                    <div class="logic-box">
                        <h4>2. 价值评估 (Critic)</h4>
                        <p>训练一个模型来打分：<br>"这步操作离成功还有多远?"</p>
                    </div>
                    <div class="arrow">➡</div>
                    <div class="logic-box">
                        <h4>3. 策略提取 (Actor)</h4>
                        <p>告诉模型：<br>"只给我生成高分的操作"</p>
                    </div>
                </div>
            </section>

            <!-- Q5: Step 1 - Value Function -->
            <section>
                <h4>原理第一步: 分布式价值函数 (The Critic)</h4>
                <p class="small-text">我们不直接预测一个标量奖励，而是预测一个<strong>分布</strong>。</p>
                <div class="math-box">
                    $$p_{\phi}(V | o_t, l) \in \Delta_B$$
                </div>
                <ul>
                    <li><strong>输入:</strong> 当前图像 $o_t$ + 语言指令 $l$。</li>
                    <li><strong>输出:</strong> 离散化的价值分布（预测任务还剩多少步成功）。</li>
                    <li><strong>目标:</strong> 最小化预测分布与实际回报 $R_t(\tau)$ 的交叉熵。</li>
                </ul>
                <p class="small-text" style="color:#888;">通俗解释：模型学会了看一眼现在的状态，就能判断"这局稳了"还是"要凉了"。</p>
            </section>

            <!-- Q5: Step 2 - Advantage -->
            <section>
                <h4>原理第二步: 优势计算 (Advantage)</h4>
                <p>有了价值函数 $V$，我们就能计算动作的<strong>优势 $A$</strong>。</p>
                <div class="math-box">
                    $$A^{\pi}(o_t, a_t) = \text{实际回报} - V^{\pi}(o_t)$$
                    或者 N-step 估计:
                    $$A^{\pi}(o_t, a_t) = (\sum r + V(o_{t+N})) - V(o_t)$$
                </div>
                <ul>
                    <li>如果 $A > 0$: 说明当前动作 $a_t$ 比平均水平好（惊喜）。</li>
                    <li>如果 $A < 0$: 说明当前动作 $a_t$ 导致了情况变差（失误）。</li>
                    <li>我们将这个 $A$ 二值化为一个指示器 $I_t$ (好/坏)。</li>
                </ul>
            </section>

            <!-- Q5: Step 3 - Policy Extraction (The Core) -->
            <section>
                <h4>原理第三步: 优势条件化 (Advantage Conditioning)</h4>
                <p class="small-text">这是 $\pi_{0.6}$ 最巧妙的地方。它没有使用复杂的梯度更新，而是通过<strong>修改输入</strong>来微调。</p>
                <p>我们基于贝叶斯规则：</p>
                <div class="math-box">
                    $$\hat{\pi}(a|o,l) \propto \pi_{ref}(a|o,l) \underbrace{\left( \frac{\pi_{ref}(a|I=\text{True}, o, l)}{\pi_{ref}(a|o,l)} \right)^\beta}_{\text{改进概率}}$$
                </div>
                <p><strong>实现方法:</strong> 在训练时，我们将 $I_t$ (优势指示符) 作为文本 Token 喂给模型。</p>
                <div style="background:#222; padding:10px; font-family:monospace; font-size:0.7em;">
                    Input: [Image] "Make Espresso" "Advantage: Positive"<br>
                    Output: [Action 1, Action 2...]
                </div>
            </section>

            <!-- Q5: Step 4 - Flow Matching -->
            <section>
                <h4>具体实现: 结合 Flow Matching</h4>
                <p>$\pi_{0.6}$ 的动作输出不是简单的回归，而是<strong>流匹配 (Flow Matching)</strong> 生成模型。</p>
                <div class="math-box">
                    $$\mathcal{L}_{total} = \mathcal{L}_{\text{text}} + \mathcal{L}_{\text{flow}}(a_{t:t+H} | I_t, o_t)$$
                </div>
                <ul>
                    <li><strong>训练时:</strong> 喂给模型好数据（$I_t$=True）和坏数据（$I_t$=False），让它学会区分。</li>
                    <li><strong>推理时:</strong> 强制输入 <code>"Advantage: Positive"</code>。</li>
                    <li><strong>CFG (无分类器引导):</strong> 推理时同时计算有条件和无条件输出，通过 $\beta$ 系数放大"好动作"的概率：
                        <br>
                        <span style="font-size:0.8em">$\text{Final Action} \approx \text{Uncond} + \beta (\text{Cond} - \text{Uncond})$</span>
                    </li>
                </ul>
            </section>

            <!-- Summary -->
            <section>
                <h3>总结: $\pi_{0.6}$ 的成功公式</h3>
                <div style="text-align:left; font-size:0.9em; margin-left: 10%;">
                    <p>1. <strong>预训练 $\pi_{0.6}^*$:</strong> 拥有海量知识。</p>
                    <p>2. <strong>部署收集:</strong> 机器人实战，包含人类接管修正。</p>
                    <p>3. <strong>Critic 评分:</strong> 训练价值函数，分辨好坏动作。</p>
                    <p>4. <strong>Actor 进化:</strong> 强制模型在该场景下只输出 "Positive Advantage" 的动作。</p>
                </div>
                <p style="margin-top:40px; color:#61dafb; font-weight:bold;">结果：做咖啡速度提升 2 倍，失败率减半。</p>
            </section>

        </div>
    </div>

    <script>
        Reveal.initialize({
            hash: true,
            math: {
                mathjax: 'https://cdn.jsdelivr.net/gh/mathjax/mathjax@2.7.8/MathJax.js',
                config: 'TeX-AMS_HTML-full',
                // pass other options into MathJax.Hub.Config()
                TeX: { Macros: { RR: "{\\bf R}" } }
            },
            plugins: [ RevealMath ]
        });
    </script>
</body>
</html>